{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define environment\n",
    "ON_KAGGLE = False\n",
    "TRAIN_PREDICT = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 1.1.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# if not ON_KAGGLE:\n",
    "#     sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "print('torch', torch.__version__)\n",
    "\n",
    "if TRAIN_PREDICT=='train':\n",
    "    from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from utils import seed_everything, set_n_get_device\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ON_KAGGLE:\n",
    "    #load utility scripts\n",
    "    pass\n",
    "\n",
    "else:#offline\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "debug = False\n",
    "SEED = 42\n",
    "IMG_HEIGHT = 137\n",
    "IMG_WIDTH = 236\n",
    "\n",
    "if TRAIN_PREDICT=='train':\n",
    "    BATCH_SIZE = 128\n",
    "else:\n",
    "    BATCH_SIZE = 256\n",
    "\n",
    "if ON_KAGGLE:\n",
    "    NUM_WORKERS = 2\n",
    "else:\n",
    "    NUM_WORKERS = 16\n",
    "\n",
    "device = set_n_get_device(\"2,3\", data_device_id=\"cuda:0\")#IMPORTANT: data_device_id is set to free gpu for storing the model, e.g.\"cuda:1\"\n",
    "multi_gpu = [0,1]\n",
    "\n",
    "if debug:\n",
    "    LOG_PATH = '../logging/v2-debug.log'\n",
    "else:\n",
    "    LOG_PATH = '../logging/v2.log'\n",
    "\n",
    "checkpoint_path = '../checkpoint/v2'\n",
    "warm_start, last_checkpoint_path = False, '../checkpoint/v2/last.pth.tar'\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "early_stopping_round = 15\n",
    "LearningRate = 5e-3\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTANTS\n",
    "n_grapheme=168\n",
    "n_vowel=11\n",
    "n_consonant=7\n",
    "#n_combo = 1295\n",
    "\n",
    "#num_classes = n_grapheme+n_vowel+n_consonant+n_combo\n",
    "num_classes = n_grapheme+n_vowel+n_consonant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "1. pytorch Dataset, data augmentation, DataLoader, train-test-split/KFold, \n",
    "2. network\n",
    "3. training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200840, 137, 236), (200840, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if ON_KAGGLE:\n",
    "    pass\n",
    "\n",
    "else:#offline\n",
    "    train_df_list = [pd.read_feather('../data/processed/train_image_data_%d.feather'%i) for i in range(4)]\n",
    "    train_images_arr = np.concatenate([df.iloc[:, 1:].values.reshape(-1, IMG_HEIGHT, IMG_WIDTH) \n",
    "                                       for df in train_df_list], axis=0)\n",
    "    train_label_df = pd.read_csv('../data/raw/train.csv')\n",
    "    train_label_arr = train_label_df[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']].values\n",
    "\n",
    "train_images_arr.shape, train_label_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 1. encode grapheme characters to index 2. use it as sampler\n",
    "unique_char = train_label_df['grapheme'].unique()\n",
    "char2ind = dict([(char,i) for i,char in enumerate(unique_char)])\n",
    "grapheme_ind = [char2ind[char] for char in train_label_df['grapheme']]\n",
    "cls_w_dict = pd.value_counts(grapheme_ind)\n",
    "cls_w_dict /= 100\n",
    "cls_w_dict = cls_w_dict.to_dict()\n",
    "cls_w = np.array([cls_w_dict[i] for i in grapheme_ind])\n",
    "\n",
    "##check onehot correct?\n",
    "#train_label_df.loc[train_label_arr[:,3]==1, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data augmentation --cutmix, mixup\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    #cut_rat = np.sqrt(1. - lam)\n",
    "    cut_rat = lam\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform, ignore edge area\n",
    "    cx = np.random.randint(W//4, W*3//4)\n",
    "    cy = np.random.randint(H//4, H*3//4)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    #print(bbx1, bby1, bbx2, bby2)\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def cutmix(data, target, alpha=1.0):\n",
    "    targets1, targets2, targets3 = target[:,0], target[:,1], target[:,2]\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets1 = targets1[indices]\n",
    "    shuffled_targets2 = targets2[indices]\n",
    "    shuffled_targets3 = targets3[indices]\n",
    "\n",
    "    #lam = np.random.beta(alpha, alpha)\n",
    "    lam = np.sqrt(np.random.rand()/4)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
    "    data[:, :, bbx1:bbx2, bby1:bby2] = data[indices, :, bbx1:bbx2, bby1:bby2]\n",
    "    #data[:, :, bbx1:bbx2, bby1:bby2] += data[indices, :, bbx1:bbx2, bby1:bby2]\n",
    "    #data = torch.clamp(data, 0, 1)\n",
    "    # adjust lambda to exactly match pixel ratio\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n",
    "\n",
    "    targets = [targets1, shuffled_targets1, targets2, shuffled_targets2, \n",
    "               targets3, shuffled_targets3, lam]\n",
    "    return data, targets\n",
    "\n",
    "def mixup(data, target, alpha=0.4):\n",
    "    targets1, targets2, targets3 = target[:,0], target[:,1], target[:,2]\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets1 = targets1[indices]\n",
    "    shuffled_targets2 = targets2[indices]\n",
    "    shuffled_targets3 = targets3[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    data = data * lam + shuffled_data * (1 - lam)\n",
    "    targets = [targets1, shuffled_targets1, targets2, shuffled_targets2, \n",
    "               targets3, shuffled_targets3, lam]\n",
    "\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf = KFold(n_splits=5, shuffle=True, random_state=SEED).split(X=train_images_arr, y=train_label_arr)\n",
    "\n",
    "# for fold, (train_idx, valid_idx) in enumerate(kf):\n",
    "    \n",
    "#     if fold in [0]:#train 1 fold for testing ideas\n",
    "#         print('========training fold %d========'%fold)\n",
    "#         print(train_idx)\n",
    "#         print(valid_idx)\n",
    "        \n",
    "#         #1.1 data\n",
    "#         train_inputs, valid_inputs = train_images_arr[train_idx], train_images_arr[valid_idx]\n",
    "#         train_outputs, valid_outputs = train_label_arr[train_idx], train_label_arr[valid_idx]\n",
    "#         #1.2 Dataset, DataLoader\n",
    "#         train_dl = prepare_dataset(train_inputs, train_outputs, mode='train', debug=debug)\n",
    "#         val_dl = prepare_dataset(valid_inputs, valid_outputs, mode='valid', debug=debug)\n",
    "\n",
    "# ####\n",
    "# for batch_id, (images, labels) in enumerate(train_dl):\n",
    "#     inputs = images.to(device=device, dtype=torch.float)\n",
    "#     truth = labels.to(device=device, dtype=torch.float)\n",
    "#     if np.random.rand()<1.0:\n",
    "#         inputs, truth = cutmix(inputs, truth, alpha=None)\n",
    "#     else:\n",
    "#         inputs, truth = mixup(inputs, truth, alpha=0.4)\n",
    "#     if batch_id==1:\n",
    "#         break\n",
    "\n",
    "# #print(inputs.shape, truth.shape)\n",
    "\n",
    "# show_inputs = inputs.cpu().numpy()[np.random.choice(BATCH_SIZE, 20, replace=False), :, :, :]\n",
    "# fig,axes = plt.subplots(5,4, figsize=(10,8))\n",
    "# for i in range(20):\n",
    "#     axes[i//4, i%4].imshow(show_inputs[i, 0], cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##crop to 128x128\n",
    "def bbox(img):\n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "    return rmin, rmax, cmin, cmax\n",
    "\n",
    "def crop_resize(img0, size=128, pad=16):\n",
    "    #crop a box around pixels large than the threshold \n",
    "    #some images contain line at the sides\n",
    "    ymin,ymax,xmin,xmax = bbox(img0[5:-5,5:-5] > 80)\n",
    "    #cropping may cut too much, so we need to add it back\n",
    "    xmin = xmin - 13 if (xmin > 13) else 0\n",
    "    ymin = ymin - 10 if (ymin > 10) else 0\n",
    "    xmax = xmax + 13 if (xmax < IMG_WIDTH - 13) else IMG_WIDTH\n",
    "    ymax = ymax + 10 if (ymax < IMG_HEIGHT - 10) else IMG_HEIGHT\n",
    "    img = img0[ymin:ymax,xmin:xmax]\n",
    "    #remove lo intensity pixels as noise\n",
    "    img[img < 28] = 0\n",
    "    lx, ly = xmax-xmin,ymax-ymin\n",
    "    l = max(lx,ly) + pad\n",
    "    #make sure that the aspect ratio is kept in rescaling\n",
    "    img = np.pad(img, [((l-ly)//2,), ((l-lx)//2,)], mode='constant')\n",
    "    return cv2.resize(img,(size,size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img0 = train_images_arr[0]\n",
    "#img0 = 255 - img0\n",
    "#img0 = (img0*(255.0/img0.max())).astype(np.uint8)\n",
    "#plt.imshow(crop_resize(rotate(img0, angle=20, reshape=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### make up a weighted/balanced data sampler --for mixup/cutmix ####\n",
    "# #weights for 168 classes--graphene_root\n",
    "# import torch.utils.data\n",
    "# _weights = [6.8,6.9,3,3.1,3,5.7,3.2,6.5,6.4,2.3,6.6,6.6,6.8,0.2,1.3,0.9,1.1,1.3,0.6,3.6,3,1.1,0.3,0.2,3,0.9,5.8,3.3,1.3,0.4,2.3,1.3,0.9,7.4,3.6,2.1,1,3.5,0.3,1.6,1.3,3.3,0.5,0.3,0.9,6.9,1.7,2.2,0.7,3.1,1.4,3.1,1.1,0.3,1.7,0.6,0.4,1.6,0.8,0.4,2.3,1.7,1.2,6.7,0.2,0.7,1.3,2.1,1.6,1.3,1,0.3,0.2,7.7,0.7,0.9,0.5,1,3.4,0.3,2.2,0.3,3.4,0.7,2.2,0.7,0.5,6,1.3,0.4,1.6,0.6,0.9,1.6,1,1.4,0.2,2.1,1.6,2.2,2.2,0.9,7.1,0.3,6.2,6.6,1.3,0.2,6.3,1.1,2.9,1.3,1.1,0.2,6.7,0.2,2.3,0.7,0.9,0.7,0.8,2.2,0.4,0.5,0.5,1.2,6.3,1.1,1.1,1,6.9,2.3,1,0.2,1.6,1.6,1,1.8,1.1,0.4,1.1,0.6,0.9,1.6,1.6,3.2,3.3,0.2,0.6,0.4,0.4,0.8,1.6,0.6,1.4,1.1,1.3,3.1,7,0.3,2.1,3.2,2.2,6.1,6.1,0.9,3.3,0.6]\n",
    "# weights_dict = dict(zip(range(len(_weights)), _weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils import set_logger, save_checkpoint, load_checkpoint\n",
    "import logging\n",
    "#import gc\n",
    "from scipy.ndimage.interpolation import rotate\n",
    "import cv2\n",
    "\n",
    "\n",
    "def prepare_dataset(img_arr, label_arr, cls_w=None, mode='train', debug=False):\n",
    "    \"\"\"\n",
    "    mode: 'train', 'valid', 'test'\n",
    "    \"\"\"\n",
    "    if debug:#for debug, sample 1/10 data\n",
    "        n = img_arr.shape[0]\n",
    "        sid = np.random.choice(n, size=n//5, replace=False)\n",
    "        img_arr = img_arr[sid]\n",
    "        label_arr = label_arr[sid]\n",
    "\n",
    "    if mode=='train':\n",
    "        ds = DatasetV1(img_arr, label_arr, mode='train', augmentation=True)\n",
    "        sampler = torch.utils.data.WeightedRandomSampler(cls_w, num_samples=len(ds), replacement=True)\n",
    "        dl = DataLoader(ds,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=False,\n",
    "                        sampler=sampler,\n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        drop_last=True\n",
    "                       )\n",
    "        return dl\n",
    "\n",
    "    elif mode=='valid':\n",
    "        ds = DatasetV1(img_arr, label_arr, mode='train', augmentation=False)\n",
    "        dl = DataLoader(ds,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=False,\n",
    "                        #sampler=sampler,\n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        drop_last=True\n",
    "                       )\n",
    "    elif mode=='test':\n",
    "        ds = DatasetV1(img_arr, label_arr, mode='test', augmentation=False)\n",
    "        dl = DataLoader(ds,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=False,\n",
    "                        #sampler=sampler,\n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        drop_last=False\n",
    "                       ) \n",
    "    return dl\n",
    "\n",
    "class DatasetV1(Dataset):\n",
    "    \"\"\"plain\"\"\"\n",
    "    def __init__(self, inputs, outputs, mode='train', augmentation=False):\n",
    "        \"\"\"\n",
    "        inputs: images, (N, H, W)\n",
    "        outputs: label, (N, 3)\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self.mode = mode \n",
    "        self.augmentation = augmentation\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #TODO: augmentation, preprocessing\n",
    "        inputs, outputs = self.inputs[idx], self.outputs[idx]\n",
    "        inputs = 255 - inputs\n",
    "        inputs = (inputs*(255.0/inputs.max())).astype(np.uint8)\n",
    "        #inputs = cv2.resize(inputs, (224, 224))#128\n",
    "        #inputs = np.clip(inputs, 0, 1)\n",
    "        if self.augmentation:\n",
    "            inputs = self.do_augmentation(inputs)\n",
    "        #crop\n",
    "        inputs = crop_resize(inputs)\n",
    "        \n",
    "        inputs = np.clip(inputs/255.0, 0, 1)\n",
    "        \n",
    "        inputs = np.expand_dims(inputs, 0)#(224,224)-->(1,224,224)\n",
    "        return inputs, outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]\n",
    "    \n",
    "    def do_augmentation(self, inputs):\n",
    "        #rotate\n",
    "        #if np.random.rand() < 0.5:\n",
    "        angle = np.random.randint(0, 40) - 20\n",
    "        inputs = rotate(inputs, angle, reshape=False)\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "def train_and_valid(net, train_dl, val_dl):\n",
    "    \"\"\"train one fold\n",
    "    \n",
    "    [settings]...\n",
    "    \n",
    "    [Epoch i]\n",
    "        [Trainset]\n",
    "            [Batch j]\n",
    "        [Validset]\n",
    "            [Batch k]\n",
    "        [Logging/Checkpoint]\n",
    "    \"\"\"\n",
    "    set_logger(LOG_PATH)\n",
    "    logging.info('\\n\\n')\n",
    "    #1. optim\n",
    "    train_params = filter(lambda p: p.requires_grad, net.parameters())\n",
    "    optimizer = torch.optim.Adam(train_params, lr=LearningRate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                          factor=0.5, patience=5, \n",
    "                                                          verbose=False, threshold=0.0001, \n",
    "                                                          threshold_mode='rel', cooldown=0, \n",
    "                                                          min_lr=0, eps=1e-08)\n",
    "    #1.1 warm-start\n",
    "    if warm_start:\n",
    "        logging.info('warm_start: '+last_checkpoint_path)\n",
    "        net, _ = load_checkpoint(last_checkpoint_path, net)\n",
    "    \n",
    "    #2. using multi GPU\n",
    "    if multi_gpu is not None:\n",
    "        net = nn.DataParallel(net, device_ids=multi_gpu)\n",
    "    #3. train\n",
    "    diff = 0\n",
    "    best_val_metric = np.inf\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i_epoch in range(NUM_EPOCHS):\n",
    "        t0 = time.time()\n",
    "        ## trainset -------------------------------------------------------------\n",
    "        net.train()\n",
    "        loss_logger = LossLogger()\n",
    "        for batch_id, (images, labels) in enumerate(train_dl):\n",
    "            inputs = images.to(device=device, dtype=torch.float)\n",
    "            truth = labels.to(device=device, dtype=torch.float)\n",
    "            \n",
    "            #do cutmix/mixup\n",
    "            if i_epoch<-1:#5\n",
    "                mode = 0\n",
    "                do_nothing = True\n",
    "            else:\n",
    "                mode = 1\n",
    "                if np.random.rand()<0.5:\n",
    "                    inputs, truth = cutmix(inputs, truth, alpha=None)\n",
    "                else:\n",
    "                    inputs, truth = mixup(inputs, truth, alpha=0.4)\n",
    "            \n",
    "            logit = net(inputs)\n",
    "            logits = torch.split(logit, [n_grapheme, n_vowel, n_consonant], dim=1)\n",
    "            train_loss = loss_logger.update(logits, truth, mode=mode)\n",
    "            #grandient accumulation step=2\n",
    "            acc_step = 1\n",
    "            if acc_step>1:\n",
    "                train_loss = train_loss / acc_step\n",
    "            train_loss.backward()\n",
    "            if (batch_id+1)%acc_step==0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        train_loss_total, _, _, _ = loss_logger.aggregate()\n",
    "        \n",
    "#         ##check for memory leakage\n",
    "#         for obj in gc.get_objects():\n",
    "#             try:\n",
    "#                 if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "#                     print(type(obj), obj.size())\n",
    "#             except:\n",
    "#                 pass\n",
    "        ## validset -------------------------------------------------------------\n",
    "        net.eval()\n",
    "        loss_logger = LossLogger()\n",
    "        metric_logger = MetricLogger()\n",
    "        with torch.no_grad():\n",
    "            for batch_id, (images, labels) in enumerate(val_dl):\n",
    "                inputs = images.to(device=device, dtype=torch.float)\n",
    "                truth = labels.to(device=device, dtype=torch.float)\n",
    "                logit = net(inputs)\n",
    "                logits = torch.split(logit, [n_grapheme, n_vowel, n_consonant], dim=1)\n",
    "                _ = loss_logger.update(logits, truth, mode=0)\n",
    "                metric_logger.update(logits, truth)\n",
    "        rec, rec_grapheme, rec_vowel, rec_consonant = metric_logger.aggregate()\n",
    "        loss_total, loss_grapheme, loss_vowel, loss_consonant = loss_logger.aggregate()\n",
    "        \n",
    "        ## callbacks -------------------------------------------------------------\n",
    "        val_metric = loss_total#rec\n",
    "        scheduler.step(val_metric)\n",
    "        \n",
    "        #sometimes too early stop, force to at least train N epochs\n",
    "        if i_epoch>=40:#-1\n",
    "            if val_metric < best_val_metric:\n",
    "                best_val_metric = val_metric\n",
    "                is_best = True\n",
    "                diff = 0\n",
    "            else:\n",
    "                is_best = False\n",
    "                diff += 1\n",
    "                if diff > early_stopping_round:\n",
    "                    logging.info('Early Stopping: val_metric does not increase %d rounds'%early_stopping_round)\n",
    "                    break\n",
    "        else:\n",
    "            is_best = False\n",
    "        \n",
    "        #save checkpoint\n",
    "        checkpoint_dict = \\\n",
    "        {\n",
    "            'epoch': i_epoch,\n",
    "            'state_dict': net.module.state_dict() if multi_gpu is not None else net.state_dict(),\n",
    "            'optim_dict' : optimizer.state_dict(),\n",
    "            'metrics': {'train_loss': train_loss_total, 'val_loss': loss_total, \n",
    "                        'val_metric': val_metric}\n",
    "        }\n",
    "        save_checkpoint(checkpoint_dict, is_best=is_best, checkpoint=checkpoint_path)\n",
    "        \n",
    "        #logging loss/metric\n",
    "        logging.info('[EPOCH %05d]train_loss: %0.4f; val_loss: %0.4f; time elapsed: %0.1f min'%(i_epoch, \n",
    "                    train_loss_total, loss_total, (time.time()-t0)/60))\n",
    "        logging.info('[valid loss]grapheme: %0.4f, vowel: %0.4f, consonant: %0.4f'%(loss_grapheme, \n",
    "                                                                        loss_vowel, loss_consonant))\n",
    "        logging.info('[valid recall]total: %0.4f, grapheme: %0.4f, vowel: %0.4f, consonant: %0.4f'%(rec, \n",
    "                    rec_grapheme, rec_vowel, rec_consonant))\n",
    "        logging.info('='*80)\n",
    "\n",
    "def predict(test_dl):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "class LossLogger(object):\n",
    "    \"\"\"loss for an epoch\n",
    "    \n",
    "    [Epoch i]:\n",
    "        loss_logger = LossLogger()\n",
    "        \n",
    "        [Batch j]:\n",
    "            loss = loss_logger.update(logits, truth)\n",
    "            loss.backward()\n",
    "        \n",
    "        loss, loss_grapheme, loss_vowel, loss_consonant = loss_logger.aggregate()\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.loss_grapheme = []\n",
    "        self.loss_vowel = []\n",
    "        self.loss_consonant = []\n",
    "\n",
    "    def update(self, logits, truth, mode=0):\n",
    "        \"\"\"\n",
    "        logits: logit splitted to [logit_grapheme, logit_vowel, logit_consonant]\n",
    "        truth: shape (N, 3)\n",
    "        \"\"\"\n",
    "        if mode==0:\n",
    "            #loss\n",
    "            loss_grapheme = F.cross_entropy(logits[0], truth[:,0].long())\n",
    "            loss_vowel = F.cross_entropy(logits[1], truth[:,1].long())\n",
    "            loss_consonant = F.cross_entropy(logits[2], truth[:,2].long())\n",
    "            loss = 0.5*loss_grapheme + 0.25*loss_vowel + 0.25*loss_consonant\n",
    "            #\n",
    "            self.loss_grapheme.append(loss_grapheme.item())\n",
    "            self.loss_vowel.append(loss_vowel.item())\n",
    "            self.loss_consonant.append(loss_consonant.item())\n",
    "            return loss\n",
    "        elif mode==1:#cutmix/mixup loss\n",
    "            truth1, truth2, truth3, truth4, truth5, truth6, lam = \\\n",
    "                    truth[0], truth[1], truth[2], truth[3], truth[4], truth[5], truth[6]\n",
    "            criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "            loss_grapheme = lam * criterion(logits[0], truth1.long()) + \\\n",
    "                (1 - lam) * criterion(logits[0], truth2.long())\n",
    "            loss_vowel = lam * criterion(logits[1], truth3.long()) + \\\n",
    "                (1 - lam) * criterion(logits[1], truth4.long())\n",
    "            loss_consonant = lam * criterion(logits[2], truth5.long()) + \\\n",
    "                (1 - lam) * criterion(logits[2], truth6.long())\n",
    "            loss = 0.5*loss_grapheme + 0.25*loss_vowel + 0.25*loss_consonant\n",
    "            #\n",
    "            self.loss_grapheme.append(loss_grapheme.item())\n",
    "            self.loss_vowel.append(loss_vowel.item())\n",
    "            self.loss_consonant.append(loss_consonant.item())\n",
    "            return loss\n",
    "    \n",
    "    def aggregate(self):\n",
    "        \"\"\"\n",
    "        for print logging\n",
    "        \"\"\"\n",
    "        loss_grapheme = np.mean(self.loss_grapheme)\n",
    "        loss_vowel = np.mean(self.loss_vowel)\n",
    "        loss_consonant = np.mean(self.loss_consonant)\n",
    "        loss_total = np.mean(\n",
    "            0.5*np.array(self.loss_grapheme) + \\\n",
    "            0.25*np.array(self.loss_vowel) + \\\n",
    "            0.25*np.array(self.loss_consonant)\n",
    "        )\n",
    "        return loss_total, loss_grapheme, loss_vowel, loss_consonant\n",
    "\n",
    "class MetricLogger(object):\n",
    "    \"\"\"recall, precision for an epoch\n",
    "    \n",
    "    [Epoch i]:\n",
    "        metric_logger = MetricLogger()\n",
    "        \n",
    "        [Batch j]:\n",
    "            metric_logger.update(logits, truth)\n",
    "        \n",
    "        rec, rec_grapheme, rec_vowel, rec_consonant = metric_logger.aggregate()\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.pred_grapheme = torch.tensor([], dtype=torch.long).cuda(device)\n",
    "        self.pred_vowel = torch.tensor([], dtype=torch.long).cuda(device=device)\n",
    "        self.pred_consonant = torch.tensor([], dtype=torch.long).cuda(device=device)\n",
    "        \n",
    "        self.truth_grapheme = torch.tensor([], dtype=torch.long).cuda(device=device)\n",
    "        self.truth_vowel = torch.tensor([], dtype=torch.long).cuda(device=device)\n",
    "        self.truth_consonant = torch.tensor([], dtype=torch.long).cuda(device=device)\n",
    "\n",
    "    def update(self, logits, truth):\n",
    "        pred = torch.argmax(logits[0], dim=1)\n",
    "        self.pred_grapheme = torch.cat([self.pred_grapheme, pred])\n",
    "        self.truth_grapheme = torch.cat([self.truth_grapheme, truth[:, 0].long()])\n",
    "        #\n",
    "        pred = torch.argmax(logits[1], dim=1)\n",
    "        self.pred_vowel = torch.cat([self.pred_vowel, pred])\n",
    "        self.truth_vowel = torch.cat([self.truth_vowel, truth[:, 1].long()])\n",
    "        #\n",
    "        pred = torch.argmax(logits[2], dim=1)\n",
    "        self.pred_consonant = torch.cat([self.pred_consonant, pred])\n",
    "        self.truth_consonant = torch.cat([self.truth_consonant, truth[:, 2].long()])\n",
    "\n",
    "    def aggregate(self):\n",
    "        rec_grapheme = recall_score(self.truth_grapheme.cpu().numpy(), \n",
    "                                    self.pred_grapheme.cpu().numpy(), \n",
    "                                    average='macro')\n",
    "        rec_vowel = recall_score(self.truth_vowel.cpu().numpy(), \n",
    "                                 self.pred_vowel.cpu().numpy(), \n",
    "                                 average='macro')\n",
    "        rec_consonant = recall_score(self.truth_consonant.cpu().numpy(), \n",
    "                                     self.pred_consonant.cpu().numpy(), \n",
    "                                     average='macro')\n",
    "        #rec = (2*rec_grapheme + 1*rec_vowel + 1*rec_consonant) / 4\n",
    "        rec = np.average([rec_grapheme, rec_vowel, rec_consonant], weights=[2,1,1])\n",
    "        return rec, rec_grapheme, rec_vowel, rec_consonant\n",
    "\n",
    "# #debug MetricLogger\n",
    "# #Epoch 0\n",
    "# loss_logger = LossLogger()\n",
    "# metric_logger = MetricLogger()\n",
    "# for batch_id, (images, labels) in enumerate(val_dl):\n",
    "#     inputs = images.to(device=device, dtype=torch.float)\n",
    "#     truth = labels.to(device=device, dtype=torch.float)\n",
    "#     if batch_id==10:\n",
    "#         break\n",
    "#     logit = net(inputs)\n",
    "#     logits = torch.split(logit, [n_grapheme, n_vowel, n_consonant], dim=1)\n",
    "#     loss = loss_logger.update(logits, truth)\n",
    "#     metric_logger.update(logits, truth)\n",
    "# rec, rec_grapheme, rec_vowel, rec_consonant = metric_logger.aggregate()\n",
    "# loss_total, loss_grapheme, loss_vowel, loss_consonant = loss_logger.aggregate()\n",
    "# print(rec, rec_grapheme, rec_vowel, rec_consonant)\n",
    "# print(loss_total, loss_grapheme, loss_vowel, loss_consonant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from senet import se_resnext50_32x4d\n",
    "from senet_v2 import se_resnext50_32x4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========training fold 0========\n",
      "[     0      1      2 ... 200835 200837 200839]\n",
      "[     4      6     12 ... 200832 200836 200838]\n",
      "model state_dict loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "[EPOCH 00000]train_loss: 2.1490; val_loss: 0.5532; time elapsed: 5.9 min\n",
      "[valid loss]grapheme: 0.8221, vowel: 0.3107, consonant: 0.2579\n",
      "[valid recall]total: 0.8203, grapheme: 0.7796, vowel: 0.9055, consonant: 0.8167\n",
      "================================================================================\n",
      "[EPOCH 00001]train_loss: 1.4241; val_loss: 0.3270; time elapsed: 5.8 min\n",
      "[valid loss]grapheme: 0.4488, vowel: 0.2250, consonant: 0.1854\n",
      "[valid recall]total: 0.9028, grapheme: 0.8672, vowel: 0.9526, consonant: 0.9243\n",
      "================================================================================\n",
      "[EPOCH 00002]train_loss: 1.2960; val_loss: 0.2591; time elapsed: 5.8 min\n",
      "[valid loss]grapheme: 0.3548, vowel: 0.1828, consonant: 0.1438\n",
      "[valid recall]total: 0.9270, grapheme: 0.8981, vowel: 0.9613, consonant: 0.9507\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-bf5264b7d7e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#3. train session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtrain_and_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-32c31481b1c8>\u001b[0m in \u001b[0;36mtrain_and_valid\u001b[0;34m(net, train_dl, val_dl)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m#grandient accumulation step=2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0macc_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0macc_step\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0macc_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### training for 5 folds here ####\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED).split(X=train_images_arr, y=train_label_arr)\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf):\n",
    "    \n",
    "    if fold in [0]:#train 1 fold for testing ideas\n",
    "        print('========training fold %d========'%fold)\n",
    "        print(train_idx)\n",
    "        print(valid_idx)\n",
    "        \n",
    "        #1.1 data\n",
    "        train_inputs, valid_inputs = train_images_arr[train_idx], train_images_arr[valid_idx]\n",
    "        train_outputs, valid_outputs = train_label_arr[train_idx], train_label_arr[valid_idx]\n",
    "        train_cls_w = cls_w[train_idx]\n",
    "        #1.2 Dataset, DataLoader\n",
    "        train_dl = prepare_dataset(train_inputs, train_outputs, cls_w=train_cls_w, mode='train', debug=debug)\n",
    "        val_dl = prepare_dataset(valid_inputs, valid_outputs, mode='valid', debug=debug)\n",
    "        \n",
    "        #2. model\n",
    "        #net = se_resnext50_32x4d(num_classes=num_classes, pretrained=None).cuda(device=device)\n",
    "        net = se_resnext50_32x4d(num_classes=num_classes, pretrained='imagenet').cuda(device=device)\n",
    "\n",
    "        #3. train session\n",
    "        train_and_valid(net, train_dl, val_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best CV: .9757 100 epochs, .9754 60 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold 0\n",
    "# [     0      1      2 ... 200835 200837 200839]\n",
    "# [     4      6     12 ... 200832 200836 200838]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check dataset/network/etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kf = KFold(n_splits=5, shuffle=True, random_state=SEED).split(X=train_images_arr, y=train_label_arr)\n",
    "\n",
    "# for fold, (train_idx, valid_idx) in enumerate(kf):\n",
    "    \n",
    "#     if fold in [0]:#train 1 fold for testing ideas\n",
    "#         print('========training fold %d========'%fold)\n",
    "#         print(train_idx)\n",
    "#         print(valid_idx)\n",
    "        \n",
    "#         #1.1 data\n",
    "#         train_inputs, valid_inputs = train_images_arr[train_idx], train_images_arr[valid_idx]\n",
    "#         train_outputs, valid_outputs = train_label_arr[train_idx], train_label_arr[valid_idx]\n",
    "#         #1.2 Dataset, DataLoader\n",
    "#         train_dl = prepare_dataset(train_inputs, train_outputs, mode='train', debug=debug)\n",
    "#         val_dl = prepare_dataset(valid_inputs, valid_outputs, mode='valid', debug=debug)\n",
    "\n",
    "# ####\n",
    "# for batch_id, (images, labels) in enumerate(train_dl):\n",
    "#     inputs = images.to(device=device, dtype=torch.float)\n",
    "#     truth = labels.to(device=device, dtype=torch.float)\n",
    "#     if np.random.rand()<1.0:\n",
    "#         inputs, truth = cutmix(inputs, truth, alpha=None)\n",
    "#     else:\n",
    "#         inputs, truth = mixup(inputs, truth, alpha=0.4)\n",
    "#     if batch_id==1:\n",
    "#         break\n",
    "\n",
    "# #print(inputs.shape, truth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_inputs = inputs.cpu().numpy()[np.random.choice(BATCH_SIZE, 20), :, :, :]\n",
    "# fig,axes = plt.subplots(5,4, figsize=(10,8))\n",
    "# for i in range(20):\n",
    "#     axes[i//4, i%4].imshow(show_inputs[i, 0], cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import senet_v2\n",
    "# importlib.reload(senet_v2)\n",
    "\n",
    "# import torch\n",
    "\n",
    "# from senet_v2 import se_resnext50_32x4d\n",
    "\n",
    "# net = se_resnext50_32x4d(num_classes=186, pretrained='imagenet', debug=True).cuda(device='cuda:2')\n",
    "\n",
    "# inputs = torch.rand((128, 1, 137, 236), dtype=torch.float).cuda(device='cuda:2')\n",
    "# print(inputs.size())\n",
    "\n",
    "# logit = net(inputs)\n",
    "# print(logit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
